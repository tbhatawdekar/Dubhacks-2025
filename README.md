# DubHacks 2025 ğŸš€

This is our hackathon project for DubHacks 2025.  
## Team
- Trisha Bhatawdekar
- Neha Dubhashi
- Misha Nivota

## Tech Stack
- [e.g. React, Node.js, Python, etc.]

## Core system

- The user sits (via webcam + mic) and conducts a mock interview â€” either a preâ€‘set question bank (e.g., â€œTell me about a time youâ€¦â€, â€œWhy do you want this role?â€) or an open prompt.

- Using audio ML you analyze: tone of voice, filler words (â€œumâ€, â€œuhâ€), pacing, volume variation, clarity of speech, confidence cues, maybe signs of nervousness (pauses, hesitation).

- Using visual ML you analyze: face (eye contact, head nodding/shaking, microâ€‘expressions, smiling), body posture (leaning in/out), gesture frequency, facial orientation (looking at camera vs away), blink rate maybe.

- Combine these into descriptive feedback: e.g., â€œYou used filler words 12 times in 90 secondsâ€, â€œYou maintained eye contact ~70% of the timeâ€, â€œYour voice volume dipped significantly in the last answerâ€, â€œYour posture is leaning back which may signal disengagementâ€ etc.

- Then provide actionable suggestions: â€œTry to reduce filler words by pausing instead of â€œumâ€ when thinkingâ€, â€œLean in slightly and keep shoulders square to cameraâ€, â€œIncrease vocal variety: you sounded quite flat through the middleâ€.

- Optionally, score the interview according to multiple axes: Confidence, Clarity, Conciseness, Engagement, Body Language. Provide benchmark/ranking vs peers.

- Provide history/dashboard: track user progress over multiple sessions, show improvements (e.g., filler words per minute dropped from 10 to 6, average eye contact from 50% to 75%, etc.).

- Offer variant modes: e.g., behavioral interview, technical interview (where you ask coding questions and they respond), roleâ€‘play with a virtual interviewer (simulate tough scenario).

- Optionally integrate language or accent support (help nonâ€‘native speakers improve clarity), or industry/domain specific (sales interviews vs engineering vs leadership).

- UI/UX design: simple webcam + mic record, playback with annotated timeline (show when filler word occurred, when gaze drifted, when posture changed). Provide transcripts of responses, highlight key phrases (maybe â€œimpactâ€‘oriented phrasingâ€, â€œSTAR techniqueâ€ compliance).


## Why itâ€™s good / what problem it solves

- Many people are nervous about interviews and donâ€™t have affordable or highâ€‘quality coaching. A semiâ€‘automated coach lowers barrier.

- Visual + audio feedback is hard to selfâ€‘get; recording yourself is okay but you may not catch your own body language or voice patterns.

- Helps quantify improvement and motivate practice (gamification, progress tracking).

- Could be used by students, jobâ€‘seekers, people switching careers, nonâ€‘native speakers.

- At hackathon scale you can build a prototype (web app + ML models) that has â€œwowâ€ factor: see yourself on camera, get instant feedback, show improvements.

- Ties into current trends: video conferencing, remote hiring, AI/ML tools for personal development.

- Technical considerations / MVP scope for hackathon

- Choose one or two strong features to build: e.g., audio fillerâ€‘word detection + basic visual gaze/posture detection.

- Use existing pretrained models for face keypoints, head orientation, gesture recognition (OpenCV, MediaPipe, etc.). For audio, maybe use speechâ€‘toâ€‘text + count filler words, plus basic voice features (pitch, volume).


### For mock questions you can have a fixed set to keep it simple.
  - Build a frontâ€‘end web interface (React) or simple Node + WebRTC for webcam/mic capture.
  - Realâ€‘time vs postâ€‘recording: postâ€‘recording is simpler for hackathon.
  - Use a dashboard with charts, scores, transcripts.
  - Make sure to cover privacy/consent (video + audio).
  - Prepare a crisp demo: show before/after improvement metric or a â€œbad vs goodâ€ session.
  - Optional: allow upload of previous session and compare sideâ€‘byâ€‘side.